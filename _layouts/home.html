{% include header.html %}

  {% include banner.html %}

  <div class="wrapper">

    {% include page-nav.html %}

  <section>
    
    <h2>Summary</h2>
    <p>LockTransformLib will be a Concurrency Library with different implementations of mutexes, condition variables, semaphores and readers-writers locks.</p> 
    <p>I will be analysing their performance along parameters such as fairness, average latency and processor utilization with implementations of different data structures using a test suite consisting of varying workloads.</p>
    <p>If time permits, I plan on switching between implementations during execution (similar to how pthread mutexes are implemented) based on observed workload - for example between a tradition lock based to a transactions based implementation.</p>

    <h2>Background</h2>

    <p>Common multi-threaded programs have areas of contention where different threads need to access or modify a shared data structure. In such cases, concurrency management primitives like locks are used. I have used those implemented for use with Pthreads and OpenMP constructs as well as implemented these in both user- and kernel-space in 15-410.</p>
    <p>When it comes to mutexes, their implementations can be broadly divided into two types:</p>

    <ul>
      <li>Blocking: Threads are descheduled while waiting</li>
      <li>Busy-wait: Threads test shared variables repeatedly</li>
    </ul>
    <p>Based on how the lock is implemented, there are varying effects on different kinds of workloads. A busy-wait implementation has much worse effects on performance and processor utilization if all the threads using the lock are on the same core vs when the thread making progress is on a different core from those waiting for the lock. Busy-wait implementation also causes more cache-coherence related traffic when threads are distributed across cores. They also perform diffenttly based on the length of the critical section.</p>
    <p>I would like to analyse different implementations and compare them against Pthread and OpenMP implementation along at least the following dimensions for each workload:</p>
    <ul>
      <li>Processor Utilization: <ul>This refers to how many (and on which cores) and how long execution contexts on cores are active. This allows us to determine power savings aspects of the workloads</ul></li>
      <li>Performance: <ul>Speed of completion of the program, and individual threads</ul></li>
      <li>Average latency for grabbing lock <ul>How long different thread have to wait for from time of requesting lock to entering the critical section</ul></li>
    </ul>
    <p>The above dimensions will be compared for workloads varying in at least the following ways:</p>
    <ul>
      <li>Length of critical sections</li>
      <li>Number of threads and lockers</li>
      <li>Distribution of threads to cores</li>
    </ul>
    <h2>The Challenge</h2>

    <p>In order to succeed at measuring along the above dimensions, I will initially need to implement the different constructs as well as design workloads that will meaningfully test them.</p>
    <p>Implementing a measuring framework while not affecting the results will also be a challenge.</p>
    <p>I plan on having at least one implementation of each primitive using transactions instead of coarse-grained locks using Intel's TS Extensions and since I have never worked with them before, that serves as one of the major challenges as well.</p>
    <p>

    <h2>Resources</h2>

    <p>I will be using the following papers and resoruces for guidance on implementation and analysis:</p>
    <ul>
      <li><a href="http://www.cs.rochester.edu/~scott/papers/1991_TOCS_synch.pdf">Algorithms for Scalable Synchronization on Shared-Memory MultiProcessors</a><li><a href="https://www-ssl.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf">Intel TSX enabling and optimization recommendations</a></li>
      <li>Starter code for data structure implementations - Linked lists, Balanced Trees for modifications</li>
    </ul>


    <h2>Goals and Deliverables</h2>

    <p>Plan to Achieve</p>

    <ul>
      <li>Implementations of each primitive with different tradeoffs</li>
      <li>One transactions based implementation for each primitive</li>
      <li>Varying data structure implementations requireing use of the above primitives (possible using open source starter code)</li>
      <li>Varying workloads that use my primitives and the data structures</li>
      <li>Measuring harness that gives meaningful information back</li>
    </ul>

    <p>Hope to Achieve</p>

    <ul>
      <li>Adaptive locks that will switch between varying implementations based on knowledge gained from above analysis</li>
      <li>Analysing memory footprint of different implementations</li>
      <li>Analysing cache coherency related traffic of the implementations</li>
    </ul>

    <h2>Platform Choice</h2>

    <p>I plan to use the Intel Xeon Phi processors for testing and measurements (available for access in the latedays cluster.) I will be implementing the library and tests in C++.</p>


    <h2>Schedule</h2>

    <table style="width:100%">
    <tr>
      <th width="20%">Week</th>
      <th width="80%">Goal</th> 
    </tr>
    <tr>
      <td width="20%">Apr 10-16</td>
      <td width="80%">Implement basic variations of primitives, data structures using primitive library, and correctness tests using data structures</td> 
    </tr>
    <tr>
      <td width="20%">Apr 17-23</td>
      <td width="80%">Implement Transactions based versions of primitives</td> 
    </tr>
    <tr>
      <td width="20%">Apr 24-30</td>
      <td width="80%">Implement different workloads and make initial measurements</td> 
    </tr>
    <tr>
      <td width="20%">May 1-7</td>
      <td width="80%">Integrate real-world benchmarks for measurements</td> 
    </tr>
    <tr>
      <td width="20%">May 8-12</td>
      <td width="80%">Buffer time or time to implement adaptive primitives</td> 
    </tr>
    </table>

    <h1>Checkpoint</h1>

    <h2>Updated Schedule</h2>


    </section>

	{{ content }}

{% include footer.html %}
